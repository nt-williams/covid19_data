---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r functions, echo=FALSE}
create_csv_url <- function(path) {
  paste0("https://raw.githubusercontent.com/nt-williams/covid19_data/master/extracted/", stringr::str_replace(path, "\\s", "%"))
}

create_file_desc <- function(path) {
  type <- stringr::str_extract(path, c("confirmed_cases", "deaths", "recovered"))
  type <- type[!is.na(type)]
  cntry <- stringr::str_extract(path, "[[:upper:]][[:alpha:]]+")
  locale <- stringr::str_extract(path, "locale")
  
  if (type == "confirmed_cases") {
    type <-  "confirmed cases"
  } else {
    type <- type
  }
  
  if (is.na(locale)) {
    loc <- glue::glue("aggregate {cntry}")
  } else {
    loc <- glue::glue("{cntry} and it's states/territories/regions")
  }
  
  glue::glue("Daily counts of {type} for the {loc}")
}
```

# COVID-19 data extraction task

<!-- badges: start -->
<!-- badges: end -->

The goal of this project is to automate daily extraction and cleaning of COVID-19 data. Currently the data processed is the publicly available data through John's Johns Hopkins University Center for Systems Science and Engineering [COVID-19 data repository](https://github.com/CSSEGISandData/COVID-19) and is parsed into more locale specific files. 

R scripts are automatically run at 12:00 PM Eastern Standard Time and cleaned data files are automatically uploaded to the `extracted` folder.

**Data file summaries**

```{r, echo=FALSE}
. <- data.frame(
  Description = purrr::map_chr(list.files(here::here("extracted")), create_file_desc),
  `File link` = purrr::map_chr(list.files(here::here("extracted")), create_csv_url)
)

knitr::kable(.)
```

